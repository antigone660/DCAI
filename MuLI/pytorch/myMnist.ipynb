{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.MNIST(root='../data',train=True,\n",
    "       transform=trans,download=True)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='../data',train=False,\n",
    "       transform=trans,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torchvision.datasets.mnist.MNIST, torch.Size([1, 28, 28]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train[0][0]),type(mnist_train),mnist_train[0][0].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "worker_num = 4\n",
    "train_iter = data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=worker_num)\n",
    "x,y = next(iter(train_iter))\n",
    "\n",
    "test_iter =data.DataLoader(mnist_test,batch_size=batch_size,shuffle=True,num_workers=worker_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([0, 1]),\n",
       " tensor(0),\n",
       " tensor([0, 2]),\n",
       " tensor([2, 4]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.reshape(torch.arange(4),(2,2))\n",
    "m,m[0],m[0][0],m[:,0],m.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5537, -0.6808, -1.0602,  ...,  2.7982,  0.9613,  0.5814],\n",
      "        [ 0.1723,  1.5856,  2.3092,  ...,  1.9295, -0.2403,  0.6322],\n",
      "        [-0.9467,  0.4239, -1.9879,  ...,  1.0999, -0.9785,  1.0164],\n",
      "        ...,\n",
      "        [ 1.7324,  0.7970, -0.4635,  ...,  1.4027,  0.5807, -1.1897],\n",
      "        [ 0.0489,  0.2303, -0.4744,  ..., -0.3455, -0.7368,  0.0964],\n",
      "        [-1.5729, -0.6324,  1.5487,  ..., -0.9316,  0.1482, -1.0869]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2408,  1.1245,  1.3638,  ...,  0.9004, -2.1179, -0.8751],\n",
      "        [-1.0295, -0.6189,  0.1638,  ..., -1.0897,  0.1559, -0.3059],\n",
      "        [-0.7629,  0.8232, -0.7335,  ..., -0.7017, -0.3502,  0.5869],\n",
      "        ...,\n",
      "        [ 0.0766,  0.1505,  0.3948,  ..., -0.3386, -1.5133,  0.0598],\n",
      "        [-0.7197, -0.3048,  1.0224,  ...,  1.0355,  0.3363,  1.3966],\n",
      "        [ 1.1644,  1.4873,  0.7481,  ..., -0.6809, -1.5262, -0.4253]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2759,  0.3769, -0.0913,  ...,  0.4476, -0.6794, -0.4663],\n",
      "        [ 0.8153, -1.2170, -0.5919,  ..., -0.1921, -1.1148, -1.0826],\n",
      "        [ 1.0865,  0.1717,  1.8116,  ..., -0.2769,  1.0519, -0.3260],\n",
      "        ...,\n",
      "        [-1.7517,  1.3008, -1.3219,  ...,  0.7507,  0.7188,  0.1351],\n",
      "        [-0.1995,  0.4047,  0.4378,  ...,  1.0736, -0.4128,  0.2861],\n",
      "        [ 1.2146,  1.9716, -1.0238,  ..., -1.0147, -0.1862, -0.0868]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,10)\n",
    ")\n",
    "\n",
    "net1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "net1.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9409,  0.3293, -0.4305,  ..., -1.8169, -0.2399, -2.1546],\n",
      "        [ 1.4246,  2.0480, -1.4935,  ..., -0.5468,  1.0160,  0.1774],\n",
      "        [ 1.6806, -0.4691, -0.6375,  ...,  1.7257,  1.3602, -0.2299],\n",
      "        ...,\n",
      "        [-0.3699,  1.6262,  1.3302,  ...,  0.0117, -0.5362,  0.6195],\n",
      "        [-0.3417,  0.6151, -1.2190,  ...,  0.3381, -0.7584,  1.5863],\n",
      "        [ 0.0969, -0.5301, -1.4670,  ...,  0.4393, -0.6664, -1.1700]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.5328,  0.4533,  0.0732,  ..., -1.4821,  0.1004, -0.9540],\n",
      "        [-0.3077,  0.8507,  0.6112,  ..., -0.7380, -0.1600, -0.0200],\n",
      "        [-1.4800,  0.5172,  1.2238,  ..., -0.3072,  0.9062, -1.0643],\n",
      "        ...,\n",
      "        [-0.3649,  2.5179,  0.9216,  ..., -1.5292, -0.0898, -1.2664],\n",
      "        [ 0.8670, -0.3942,  0.0032,  ..., -0.9505, -0.3783, -0.9162],\n",
      "        [-1.3813,  0.5354, -2.1080,  ..., -0.3722, -0.6903,  0.4675]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "RELU NET1 epoch:0 train acc:tensor(0.8647, device='cuda:0') test acc:tensor(0.9132, device='cuda:0')\n",
      "RELU NET1 epoch:1 train acc:tensor(0.9206, device='cuda:0') test acc:tensor(0.9253, device='cuda:0')\n",
      "RELU NET1 epoch:2 train acc:tensor(0.9357, device='cuda:0') test acc:tensor(0.9337, device='cuda:0')\n",
      "RELU NET1 epoch:3 train acc:tensor(0.9444, device='cuda:0') test acc:tensor(0.9310, device='cuda:0')\n",
      "RELU NET1 epoch:4 train acc:tensor(0.9510, device='cuda:0') test acc:tensor(0.9396, device='cuda:0')\n",
      "RELU NET1 epoch:5 train acc:tensor(0.9564, device='cuda:0') test acc:tensor(0.9382, device='cuda:0')\n",
      "RELU NET1 epoch:6 train acc:tensor(0.9596, device='cuda:0') test acc:tensor(0.9393, device='cuda:0')\n",
      "RELU NET1 epoch:7 train acc:tensor(0.9644, device='cuda:0') test acc:tensor(0.9387, device='cuda:0')\n",
      "RELU NET1 epoch:8 train acc:tensor(0.9660, device='cuda:0') test acc:tensor(0.9414, device='cuda:0')\n",
      "RELU NET1 epoch:9 train acc:tensor(0.9683, device='cuda:0') test acc:tensor(0.9436, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    "    \n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net1.to(device)         #加载模型到GPU上\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "net1.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()                                #定义损失函数为交叉熵\n",
    "trainer = torch.optim.SGD(net1.parameters(),lr=0.1)         #此时trainner已经知道网络的结构以及如何反向求导\n",
    "\n",
    "acc_l = []\n",
    "epoch_num = 10\n",
    "for epoch in range(epoch_num):\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    for x,y in train_iter:\n",
    "        x = x.to(device)        #将数据加载到dataloader上\n",
    "        y = y.to(device)\n",
    "        y_hat = net1(x)\n",
    "        l = loss(y_hat,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        y_hat = y_hat.argmax(axis = 1)\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        positive_num = cmp.sum()\n",
    "        pos_num += positive_num\n",
    "        y_num += y.numel()\n",
    "    acc = pos_num / y_num\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_iter:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net1(x)\n",
    "            y_hat = y_hat.argmax(axis=1)\n",
    "            cmp = y_hat.type(y.dtype) == y\n",
    "            pos_num += cmp.sum()\n",
    "            y_num += y.numel()\n",
    "\n",
    "        t_acc = pos_num / y_num\n",
    "    print(\"RELU NET1 epoch:\"+ str(epoch)+\" train acc:\"+str(acc)+\" test acc:\" + str(t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000266CF1A1AF0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.9266, -0.3972,  0.8666,  ...,  1.0585, -0.4584, -1.5465],\n",
      "        [-0.8435, -2.3588, -0.0224,  ...,  0.0211,  1.0834, -2.1053],\n",
      "        [-0.5773,  0.1940, -0.7952,  ...,  0.5701,  1.0039,  1.0473],\n",
      "        ...,\n",
      "        [-1.0100, -0.6467,  0.0424,  ...,  1.3465, -0.5013, -1.0591],\n",
      "        [ 1.1799, -0.9767, -2.4815,  ..., -1.0819,  0.4569,  0.4636],\n",
      "        [ 0.8420,  0.4215,  0.0161,  ..., -0.4479, -0.3205,  1.4035]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6155, -0.9290,  0.2478,  ...,  1.4076, -0.3018,  1.9793],\n",
      "        [-1.8361,  0.2102, -0.6052,  ..., -1.2891,  0.6505,  0.4345],\n",
      "        [ 1.6620,  0.3181,  0.6589,  ...,  0.5669, -1.5309,  0.3010],\n",
      "        ...,\n",
      "        [ 0.8252,  1.0049, -0.4161,  ..., -1.5556, -0.1481, -0.3990],\n",
      "        [ 2.8029, -1.2911, -0.5146,  ...,  0.3455,  0.3947, -1.3327],\n",
      "        [-0.3449, -1.6282,  0.5144,  ...,  1.0927,  0.0724, -1.5654]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "NET1 epoch:0 train acc:tensor(0.8449, device='cuda:0') test acc:tensor(0.8704, device='cuda:0')\n",
      "NET1 epoch:1 train acc:tensor(0.8754, device='cuda:0') test acc:tensor(0.8779, device='cuda:0')\n",
      "NET1 epoch:2 train acc:tensor(0.8774, device='cuda:0') test acc:tensor(0.8835, device='cuda:0')\n",
      "NET1 epoch:3 train acc:tensor(0.8787, device='cuda:0') test acc:tensor(0.8592, device='cuda:0')\n",
      "NET1 epoch:4 train acc:tensor(0.8770, device='cuda:0') test acc:tensor(0.8742, device='cuda:0')\n",
      "NET1 epoch:5 train acc:tensor(0.8784, device='cuda:0') test acc:tensor(0.8886, device='cuda:0')\n",
      "NET1 epoch:6 train acc:tensor(0.8795, device='cuda:0') test acc:tensor(0.8740, device='cuda:0')\n",
      "NET1 epoch:7 train acc:tensor(0.8788, device='cuda:0') test acc:tensor(0.8761, device='cuda:0')\n",
      "NET1 epoch:8 train acc:tensor(0.8806, device='cuda:0') test acc:tensor(0.8698, device='cuda:0')\n",
      "NET1 epoch:9 train acc:tensor(0.8800, device='cuda:0') test acc:tensor(0.8683, device='cuda:0')\n",
      "NET1 epoch:10 train acc:tensor(0.8805, device='cuda:0') test acc:tensor(0.8712, device='cuda:0')\n",
      "NET1 epoch:11 train acc:tensor(0.8802, device='cuda:0') test acc:tensor(0.8868, device='cuda:0')\n",
      "NET1 epoch:12 train acc:tensor(0.8825, device='cuda:0') test acc:tensor(0.8766, device='cuda:0')\n",
      "NET1 epoch:13 train acc:tensor(0.8809, device='cuda:0') test acc:tensor(0.8822, device='cuda:0')\n",
      "NET1 epoch:14 train acc:tensor(0.8821, device='cuda:0') test acc:tensor(0.8644, device='cuda:0')\n",
      "NET1 epoch:15 train acc:tensor(0.8822, device='cuda:0') test acc:tensor(0.8649, device='cuda:0')\n",
      "NET1 epoch:16 train acc:tensor(0.8820, device='cuda:0') test acc:tensor(0.8709, device='cuda:0')\n",
      "NET1 epoch:17 train acc:tensor(0.8824, device='cuda:0') test acc:tensor(0.8826, device='cuda:0')\n",
      "NET1 epoch:18 train acc:tensor(0.8837, device='cuda:0') test acc:tensor(0.8814, device='cuda:0')\n",
      "NET1 epoch:19 train acc:tensor(0.8839, device='cuda:0') test acc:tensor(0.8829, device='cuda:0')\n",
      "NET1 epoch:20 train acc:tensor(0.8848, device='cuda:0') test acc:tensor(0.8805, device='cuda:0')\n",
      "NET1 epoch:21 train acc:tensor(0.8847, device='cuda:0') test acc:tensor(0.8751, device='cuda:0')\n",
      "NET1 epoch:22 train acc:tensor(0.8829, device='cuda:0') test acc:tensor(0.8791, device='cuda:0')\n",
      "NET1 epoch:23 train acc:tensor(0.8855, device='cuda:0') test acc:tensor(0.8819, device='cuda:0')\n",
      "NET1 epoch:24 train acc:tensor(0.8843, device='cuda:0') test acc:tensor(0.8725, device='cuda:0')\n",
      "NET1 epoch:25 train acc:tensor(0.8843, device='cuda:0') test acc:tensor(0.8953, device='cuda:0')\n",
      "NET1 epoch:26 train acc:tensor(0.8855, device='cuda:0') test acc:tensor(0.8695, device='cuda:0')\n",
      "NET1 epoch:27 train acc:tensor(0.8858, device='cuda:0') test acc:tensor(0.8773, device='cuda:0')\n",
      "NET1 epoch:28 train acc:tensor(0.8859, device='cuda:0') test acc:tensor(0.8872, device='cuda:0')\n",
      "NET1 epoch:29 train acc:tensor(0.8840, device='cuda:0') test acc:tensor(0.8730, device='cuda:0')\n",
      "NET1 epoch:30 train acc:tensor(0.8859, device='cuda:0') test acc:tensor(0.8734, device='cuda:0')\n",
      "NET1 epoch:31 train acc:tensor(0.8859, device='cuda:0') test acc:tensor(0.8832, device='cuda:0')\n",
      "NET1 epoch:32 train acc:tensor(0.8868, device='cuda:0') test acc:tensor(0.8807, device='cuda:0')\n",
      "NET1 epoch:33 train acc:tensor(0.8848, device='cuda:0') test acc:tensor(0.8821, device='cuda:0')\n",
      "NET1 epoch:34 train acc:tensor(0.8863, device='cuda:0') test acc:tensor(0.8737, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,256),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net1.to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "net1.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net1.parameters(),lr=0.1)\n",
    "\n",
    "acc_l = []\n",
    "epoch_num = 35\n",
    "for epoch in range(epoch_num):\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    for x,y in train_iter:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net1(x)\n",
    "        l = loss(y_hat,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        y_hat = y_hat.argmax(axis = 1)\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        positive_num = cmp.sum()\n",
    "        pos_num += positive_num\n",
    "        y_num += y.numel()\n",
    "    acc = pos_num / y_num\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_iter:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net1(x)\n",
    "            y_hat = y_hat.argmax(axis=1)\n",
    "            cmp = y_hat.type(y.dtype) == y\n",
    "            pos_num += cmp.sum()\n",
    "            y_num += y.numel()\n",
    "\n",
    "        t_acc = pos_num / y_num\n",
    "    print(\"NET1 epoch:\"+ str(epoch)+\" train acc:\"+str(acc)+\" test acc:\" + str(t_acc))\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0383,  0.8840,  2.1829,  ...,  0.2992,  1.7481,  1.0355],\n",
      "        [-1.3725, -0.1229,  0.3933,  ...,  0.1188, -0.7940, -1.1025],\n",
      "        [-0.6111,  0.7120,  0.2814,  ..., -1.0841, -1.3800,  0.3569],\n",
      "        ...,\n",
      "        [-0.2812, -2.2271, -0.3444,  ..., -1.9794, -0.4506,  0.1355],\n",
      "        [ 2.0004, -0.7227, -0.8992,  ...,  0.4950,  0.2703,  0.0983],\n",
      "        [-0.6151, -0.3467, -0.6199,  ..., -1.9513, -0.4260, -0.7879]],\n",
      "       requires_grad=True)\n",
      "epoch:0 train acc:tensor(0.6550) test acc:tensor(0.8012)\n",
      "epoch:1 train acc:tensor(0.8166) test acc:tensor(0.8432)\n",
      "epoch:2 train acc:tensor(0.8453) test acc:tensor(0.8620)\n",
      "epoch:3 train acc:tensor(0.8586) test acc:tensor(0.8672)\n",
      "epoch:4 train acc:tensor(0.8670) test acc:tensor(0.8760)\n",
      "epoch:5 train acc:tensor(0.8733) test acc:tensor(0.8791)\n",
      "epoch:6 train acc:tensor(0.8774) test acc:tensor(0.8840)\n",
      "epoch:7 train acc:tensor(0.8810) test acc:tensor(0.8876)\n",
      "epoch:8 train acc:tensor(0.8844) test acc:tensor(0.8908)\n",
      "epoch:9 train acc:tensor(0.8879) test acc:tensor(0.8921)\n",
      "epoch:10 train acc:tensor(0.8900) test acc:tensor(0.8920)\n",
      "epoch:11 train acc:tensor(0.8917) test acc:tensor(0.8955)\n",
      "epoch:12 train acc:tensor(0.8939) test acc:tensor(0.8962)\n",
      "epoch:13 train acc:tensor(0.8961) test acc:tensor(0.8975)\n",
      "epoch:14 train acc:tensor(0.8976) test acc:tensor(0.9003)\n",
      "epoch:15 train acc:tensor(0.8987) test acc:tensor(0.9025)\n",
      "epoch:16 train acc:tensor(0.9000) test acc:tensor(0.9027)\n",
      "epoch:17 train acc:tensor(0.9010) test acc:tensor(0.9026)\n",
      "epoch:18 train acc:tensor(0.9022) test acc:tensor(0.9043)\n",
      "epoch:19 train acc:tensor(0.9032) test acc:tensor(0.9062)\n",
      "epoch:20 train acc:tensor(0.9047) test acc:tensor(0.9060)\n",
      "epoch:21 train acc:tensor(0.9058) test acc:tensor(0.9076)\n",
      "epoch:22 train acc:tensor(0.9069) test acc:tensor(0.9066)\n",
      "epoch:23 train acc:tensor(0.9076) test acc:tensor(0.9083)\n",
      "epoch:24 train acc:tensor(0.9082) test acc:tensor(0.9088)\n",
      "epoch:25 train acc:tensor(0.9097) test acc:tensor(0.9085)\n",
      "epoch:26 train acc:tensor(0.9101) test acc:tensor(0.9110)\n",
      "epoch:27 train acc:tensor(0.9108) test acc:tensor(0.9108)\n",
      "epoch:28 train acc:tensor(0.9117) test acc:tensor(0.9110)\n",
      "epoch:29 train acc:tensor(0.9122) test acc:tensor(0.9122)\n",
      "epoch:30 train acc:tensor(0.9122) test acc:tensor(0.9117)\n",
      "epoch:31 train acc:tensor(0.9135) test acc:tensor(0.9126)\n",
      "epoch:32 train acc:tensor(0.9137) test acc:tensor(0.9122)\n",
      "epoch:33 train acc:tensor(0.9140) test acc:tensor(0.9147)\n",
      "epoch:34 train acc:tensor(0.9141) test acc:tensor(0.9136)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,10)\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "\n",
    "acc_l = []\n",
    "epoch_num = 35\n",
    "for epoch in range(epoch_num):\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    for x,y in train_iter:\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        y_hat = y_hat.argmax(axis = 1)\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        positive_num = cmp.sum()\n",
    "        pos_num += positive_num\n",
    "        y_num += y.numel()\n",
    "    acc = pos_num / y_num\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_iter:\n",
    "            y_hat = net(x)\n",
    "            y_hat = y_hat.argmax(axis=1)\n",
    "            cmp = y_hat.type(y.dtype) == y\n",
    "            pos_num += cmp.sum()\n",
    "            y_num += y.numel()\n",
    "\n",
    "        t_acc = pos_num / y_num\n",
    "    print(\"epoch:\"+ str(epoch)+\" train acc:\"+str(acc)+\" test acc:\" + str(t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.6774,  0.2920, -0.3748,  ...,  0.9559,  0.1662,  0.1861],\n",
      "        [-0.8039, -0.2686,  0.3063,  ...,  0.3829, -0.6849, -1.2516],\n",
      "        [ 0.5782, -1.1185,  1.1356,  ..., -1.3706, -0.6860, -0.6960],\n",
      "        ...,\n",
      "        [-0.3401, -1.4149, -0.5510,  ...,  0.5118, -0.9675,  0.0607],\n",
      "        [ 0.3590, -0.7676,  0.0838,  ...,  1.2927, -1.4843, -0.1991],\n",
      "        [-0.3457, -0.1029, -0.5950,  ..., -0.9965,  0.7509, -0.5314]],\n",
      "       requires_grad=True)\n",
      "epoch:0 train acc:tensor(0.8358) test acc:tensor(0.8784)\n",
      "epoch:1 train acc:tensor(0.8867) test acc:tensor(0.8908)\n",
      "epoch:2 train acc:tensor(0.8968) test acc:tensor(0.9045)\n",
      "epoch:3 train acc:tensor(0.9010) test acc:tensor(0.9109)\n",
      "epoch:4 train acc:tensor(0.9058) test acc:tensor(0.9003)\n",
      "epoch:5 train acc:tensor(0.9094) test acc:tensor(0.9092)\n",
      "epoch:6 train acc:tensor(0.9108) test acc:tensor(0.9075)\n",
      "epoch:7 train acc:tensor(0.9115) test acc:tensor(0.9063)\n",
      "epoch:8 train acc:tensor(0.9134) test acc:tensor(0.8946)\n",
      "epoch:9 train acc:tensor(0.9121) test acc:tensor(0.9184)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,10)\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(),lr=1)\n",
    "\n",
    "\n",
    "acc_l = []\n",
    "epoch_num = 10\n",
    "for epoch in range(epoch_num):\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    for x,y in train_iter:\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        y_hat = y_hat.argmax(axis = 1)\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        positive_num = cmp.sum()\n",
    "        pos_num += positive_num\n",
    "        y_num += y.numel()\n",
    "    acc = pos_num / y_num\n",
    "    y_num = 0\n",
    "    pos_num = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_iter:\n",
    "            y_hat = net(x)\n",
    "            y_hat = y_hat.argmax(axis=1)\n",
    "            cmp = y_hat.type(y.dtype) == y\n",
    "            pos_num += cmp.sum()\n",
    "            y_num += y.numel()\n",
    "\n",
    "        t_acc = pos_num / y_num\n",
    "    print(\"epoch:\"+ str(epoch)+\" train acc:\"+str(acc)+\" test acc:\" + str(t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输入\n",
      "torch.Size([1, 28, 28])\n",
      "原始输出\n",
      "tensor([[ -2.9996,  -9.2320,   0.6083,   9.7574, -11.0672,  11.5504, -10.3458,\n",
      "          -0.0734,  -0.4477,  -1.0974]]) \n",
      "概率化输出\n",
      "tensor([[4.1129e-07, 8.0809e-10, 1.5172e-05, 1.4271e-01, 1.2896e-10, 8.5726e-01,\n",
      "         2.6530e-10, 7.6728e-06, 5.2772e-06, 2.7559e-06]])\n",
      "概率求和\n",
      "tensor(1.)\n",
      "识别结果\n",
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGxCAYAAADLfglZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcRklEQVR4nO3df3DU953f8dcawRq41boKSLsKQlEdqD1IIQ0QfhwG4QYZdcwY46TY7mQgTTz+IbihwvUF0ylcJod8dmHIBRs3nhyGBAKTG/+ghdpWiiXMYLmYk21KXCwOEZRDQkU2u0LGAolP/6BsvYAhn/Uub630fMzsjLX7ffP9+Ouv/fRXu/oq4JxzAgDAwE3WCwAADFxECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAi4Aerq6hQIBK76aGhosF4eYCbHegHAQLJ69WrNmjUr6bnS0lKj1QD2iBBwA40ZM0ZTpkyxXgbQZ/DtOACAGSIE3EBVVVXKyclRbm6u7rrrLu3du9d6SYCpAL/KAci8xsZGbdq0SeXl5frKV76iI0eO6JlnntFHH32knTt36q677rJeImCCCAFGTp8+rbKyMuXl5en999+3Xg5ggm/HAUZuueUW3X333frggw909uxZ6+UAJogQYOjSNyICgYDxSgAbfDsOMPLJJ5+orKxMI0eOVGNjo/VyABP8nBBwAzz44IMaPXq0Jk6cqBEjRqipqUlr1qzRyZMn9eKLL1ovDzBDhIAb4Bvf+Ia2b9+u559/XmfOnFFeXp6mT5+uX/3qV5o0aZL18gAzfDsOAGCGDyYAAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOlzPyd04cIFnThxQqFQiFuZAEAWcs6ps7NThYWFuumma1/r9LkInThxQkVFRdbLAAB8SS0tLRo1atQ1t+lzEQqFQpKk6frXytFg49UAAHz16Lz2alfiv+fXkrEIPffcc3rmmWfU2tqqcePGad26dbrjjjuuO3fpW3A5GqycABECgKzz/+7D86e8pZKRDyZs375dS5cu1YoVK9TY2Kg77rhDlZWVOn78eCZ2BwDIUhmJ0Nq1a/XDH/5QP/rRj3T77bdr3bp1Kioq0oYNGzKxOwBAlkp7hM6dO6cDBw6ooqIi6fmKigrt27fviu27u7sVj8eTHgCAgSHtETp16pR6e3tVUFCQ9HxBQYHa2tqu2L6mpkbhcDjx4JNxADBwZOyHVS9/Q8o5d9U3qZYvX65YLJZ4tLS0ZGpJAIA+Ju2fjhsxYoQGDRp0xVVPe3v7FVdHkhQMBhUMBtO9DABAFkj7ldCQIUM0YcIE1dbWJj1fW1uradOmpXt3AIAslpGfE6qurtb3v/99TZw4UVOnTtUvfvELHT9+XI888kgmdgcAyFIZidCCBQvU0dGhn/zkJ2ptbVVpaal27dql4uLiTOwOAJClAs45Z72Iz4vH4wqHwyrXPdwxAQCyUI87rzq9qlgsptzc3Gtuy69yAACYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMzkWC8A6EsCOf7/SgwaOSIDK0mPw49/LaW53mEXvGeKb233nhn2WMB7pm3tEO+Zf5i43XtGkk71dnnPTP7tMu+Zr1c3eM/0F1wJAQDMECEAgJm0R2jVqlUKBAJJj0gkku7dAAD6gYy8JzRu3Dj97ne/S3w9aNCgTOwGAJDlMhKhnJwcrn4AANeVkfeEmpqaVFhYqJKSEt1///06evToF27b3d2teDye9AAADAxpj9DkyZO1efNmvf7663rhhRfU1tamadOmqaOj46rb19TUKBwOJx5FRUXpXhIAoI9Ke4QqKyt13333qaysTN/5zne0c+dOSdKmTZuuuv3y5csVi8USj5aWlnQvCQDQR2X8h1WHDx+usrIyNTU1XfX1YDCoYDCY6WUAAPqgjP+cUHd3tz788ENFo9FM7woAkGXSHqHHH39c9fX1am5u1jvvvKPvfve7isfjWrhwYbp3BQDIcmn/dtwf//hHPfDAAzp16pRGjhypKVOmqKGhQcXFxeneFQAgy6U9Qtu2bUv3H4k+atDtY7xnXHCw98yJmbd4z5yd4n/jSUnKC/vPvTU+tZtj9jf//dOQ98zfrJ/jPfNO2VbvmebzZ71nJOmpk7O9Zwrfcinta6Di3nEAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJmM/1I79H295d9KaW7ti896z4wdPCSlfeHGOu96vWf+088Xec/kdPnf7HPqbxd7z4T+qcd7RpKCp/xvfDrs3XdS2tdAxZUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHAXbSh4+ERKcwc+K/KeGTv4ZEr76m+WtU7xnjl6ZoT3zIu3/r33jCTFLvjf3brgb/eltK++zP8owBdXQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGW5gCvW0tqU09/O/+Z73zF/P6fKeGfTBn3nPvP/Yz71nUvXTU9/wnjnynWHeM72nW71nHpz6mPeMJB37C/+ZEr2f0r4wsHElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamSFnexre9Z0b+1694z/R2fOw9M67033nPSNKhGX/nPbPjFzO9Z/JP7/OeSUXg7dRuKlri/48WSAlXQgAAM0QIAGDGO0J79uzR3LlzVVhYqEAgoFdeeSXpdeecVq1apcLCQg0dOlTl5eU6dOhQutYLAOhHvCPU1dWl8ePHa/369Vd9/emnn9batWu1fv167d+/X5FIRLNnz1ZnZ+eXXiwAoH/x/mBCZWWlKisrr/qac07r1q3TihUrNH/+fEnSpk2bVFBQoK1bt+rhhx/+cqsFAPQraX1PqLm5WW1tbaqoqEg8FwwGNXPmTO3bd/VPA3V3dysejyc9AAADQ1oj1NbWJkkqKChIer6goCDx2uVqamoUDocTj6KionQuCQDQh2Xk03GBQCDpa+fcFc9dsnz5csViscSjpaUlE0sCAPRBaf1h1UgkIuniFVE0Gk08397efsXV0SXBYFDBYDCdywAAZIm0XgmVlJQoEomotrY28dy5c+dUX1+vadOmpXNXAIB+wPtK6MyZMzpy5Eji6+bmZr333nvKy8vT6NGjtXTpUq1evVpjxozRmDFjtHr1ag0bNkwPPvhgWhcOAMh+3hF69913NWvWrMTX1dXVkqSFCxfqxRdf1BNPPKGzZ8/qscce0yeffKLJkyfrjTfeUCgUSt+qAQD9QsA556wX8XnxeFzhcFjlukc5gcHWy0GW+ui/TEpt7u7nvWd+8Id/5T3zf6an8MPbF3r9ZwADPe686vSqYrGYcnNzr7kt944DAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmbT+ZlWgr7j9Lz9Kae4HZf53xN5Y/D+8Z2Z+r8p7JrS9wXsG6Ou4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHADU/RLvadjKc11PHq798zxHWe9Z378083eM8v/zb3eM64x7D0jSUV//bb/kHMp7QsDG1dCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZbmAKfM6F9z/0nrn/r/6D98yWlf/Ze+a9Kf43PdUU/xFJGjd8sffMmBdavWd6jh7znkH/wpUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGAm4Jxz1ov4vHg8rnA4rHLdo5zAYOvlABnh/vyb3jO5T/3Re+Y3//x175lU3fbmj7xn/sVfxbxnepuOes/gxupx51WnVxWLxZSbm3vNbbkSAgCYIUIAADPeEdqzZ4/mzp2rwsJCBQIBvfLKK0mvL1q0SIFAIOkxZUqKv9QEANCveUeoq6tL48eP1/r1679wmzlz5qi1tTXx2LVr15daJACgf/L+zaqVlZWqrKy85jbBYFCRSCTlRQEABoaMvCdUV1en/Px8jR07Vg899JDa29u/cNvu7m7F4/GkBwBgYEh7hCorK7Vlyxbt3r1ba9as0f79+3XnnXequ7v7qtvX1NQoHA4nHkVFReleEgCgj/L+dtz1LFiwIPHXpaWlmjhxooqLi7Vz507Nnz//iu2XL1+u6urqxNfxeJwQAcAAkfYIXS4ajaq4uFhNTU1XfT0YDCoYDGZ6GQCAPijjPyfU0dGhlpYWRaPRTO8KAJBlvK+Ezpw5oyNHjiS+bm5u1nvvvae8vDzl5eVp1apVuu+++xSNRnXs2DE9+eSTGjFihO699960LhwAkP28I/Tuu+9q1qxZia8vvZ+zcOFCbdiwQQcPHtTmzZt1+vRpRaNRzZo1S9u3b1coFErfqgEA/QI3MAWyxKCCfO+ZEwu+ntK+3vnLn3nP3JTCd/f/bXOF90xseof3DG4sbmAKAMgKRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMJPx36wKID16T7Z7zxT8rf+MJH32RI/3zLDAEO+ZF77237xn7r53qffMsJff8Z7BjcGVEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghhuYAgYuTP+m98w/fu9m75nSbx7znpFSuxlpKn7+8b/0nhn26rsZWAmscCUEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhBqbA5wQmlnrPfPQX/jf7fOHPN3nPzLj5nPfMjdTtznvPNHxc4r+jC63+M+izuBICAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMxwA1P0eTklxd4z//iDwpT2tWrBNu+Z+/7sVEr76suePDnRe6b+Z1O8Z/7Zpre9Z9C/cCUEADBDhAAAZrwiVFNTo0mTJikUCik/P1/z5s3T4cOHk7ZxzmnVqlUqLCzU0KFDVV5erkOHDqV10QCA/sErQvX19aqqqlJDQ4Nqa2vV09OjiooKdXV1JbZ5+umntXbtWq1fv1779+9XJBLR7Nmz1dnZmfbFAwCym9cHE1577bWkrzdu3Kj8/HwdOHBAM2bMkHNO69at04oVKzR//nxJ0qZNm1RQUKCtW7fq4YcfTt/KAQBZ70u9JxSLxSRJeXl5kqTm5ma1tbWpoqIisU0wGNTMmTO1b9++q/4Z3d3disfjSQ8AwMCQcoScc6qurtb06dNVWloqSWpra5MkFRQUJG1bUFCQeO1yNTU1CofDiUdRUVGqSwIAZJmUI7R48WJ98MEH+s1vfnPFa4FAIOlr59wVz12yfPlyxWKxxKOlpSXVJQEAskxKP6y6ZMkS7dixQ3v27NGoUaMSz0ciEUkXr4ii0Wji+fb29iuuji4JBoMKBoOpLAMAkOW8roScc1q8eLFeeukl7d69WyUlJUmvl5SUKBKJqLa2NvHcuXPnVF9fr2nTpqVnxQCAfsPrSqiqqkpbt27Vq6++qlAolHifJxwOa+jQoQoEAlq6dKlWr16tMWPGaMyYMVq9erWGDRumBx98MCN/AwCA7OUVoQ0bNkiSysvLk57fuHGjFi1aJEl64okndPbsWT322GP65JNPNHnyZL3xxhsKhUJpWTAAoP8IOOec9SI+Lx6PKxwOq1z3KCcw2Ho5uIacr432nolNiF5/o8ss+Mlr19/oMo/cctR7pq9b1up/g9C3n/O/Eakk5b34P/2HLvSmtC/0Pz3uvOr0qmKxmHJzc6+5LfeOAwCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJmUfrMq+q6caMR75uO/G57Svh4tqfeeeSB0MqV99WWL/2m698w/bPim98yIv/9f3jN5nW97zwA3EldCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZbmB6g5y7a6L/zL//2Hvmya/v8p6pGNrlPdPXnew9m9LcjB3LvGdu+4//23sm77T/jUUveE8AfR9XQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGW5geoMcm+ff+4/KfpuBlaTPs6dv9Z75WX2F90ygN+A9c9tPm71nJGnMyXe8Z3pT2hMAiSshAIAhIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMBMwDnnrBfxefF4XOFwWOW6RzmBwdbLAQB46nHnVadXFYvFlJube81tuRICAJghQgAAM14Rqqmp0aRJkxQKhZSfn6958+bp8OHDSdssWrRIgUAg6TFlypS0LhoA0D94Rai+vl5VVVVqaGhQbW2tenp6VFFRoa6urqTt5syZo9bW1sRj165daV00AKB/8PrNqq+99lrS1xs3blR+fr4OHDigGTNmJJ4PBoOKRCLpWSEAoN/6Uu8JxWIxSVJeXl7S83V1dcrPz9fYsWP10EMPqb29/Qv/jO7ubsXj8aQHAGBgSDlCzjlVV1dr+vTpKi0tTTxfWVmpLVu2aPfu3VqzZo3279+vO++8U93d3Vf9c2pqahQOhxOPoqKiVJcEAMgyKf+cUFVVlXbu3Km9e/dq1KhRX7hda2uriouLtW3bNs2fP/+K17u7u5MCFY/HVVRUxM8JAUCW8vk5Ia/3hC5ZsmSJduzYoT179lwzQJIUjUZVXFyspqamq74eDAYVDAZTWQYAIMt5Rcg5pyVLlujll19WXV2dSkpKrjvT0dGhlpYWRaPRlBcJAOifvN4Tqqqq0q9//Wtt3bpVoVBIbW1tamtr09mzZyVJZ86c0eOPP663335bx44dU11dnebOnasRI0bo3nvvzcjfAAAge3ldCW3YsEGSVF5envT8xo0btWjRIg0aNEgHDx7U5s2bdfr0aUWjUc2aNUvbt29XKBRK26IBAP2D97fjrmXo0KF6/fXXv9SCAAADB/eOAwCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYybFewOWcc5KkHp2XnPFiAADeenRe0v//7/m19LkIdXZ2SpL2apfxSgAAX0ZnZ6fC4fA1twm4PyVVN9CFCxd04sQJhUIhBQKBpNfi8biKiorU0tKi3NxcoxXa4zhcxHG4iONwEcfhor5wHJxz6uzsVGFhoW666drv+vS5K6GbbrpJo0aNuuY2ubm5A/oku4TjcBHH4SKOw0Uch4usj8P1roAu4YMJAAAzRAgAYCarIhQMBrVy5UoFg0HrpZjiOFzEcbiI43ARx+GibDsOfe6DCQCAgSOrroQAAP0LEQIAmCFCAAAzRAgAYIYIAQDMZFWEnnvuOZWUlOjmm2/WhAkT9NZbb1kv6YZatWqVAoFA0iMSiVgvK+P27NmjuXPnqrCwUIFAQK+88krS6845rVq1SoWFhRo6dKjKy8t16NAhm8Vm0PWOw6JFi644P6ZMmWKz2AypqanRpEmTFAqFlJ+fr3nz5unw4cNJ2wyE8+FPOQ7Zcj5kTYS2b9+upUuXasWKFWpsbNQdd9yhyspKHT9+3HppN9S4cePU2tqaeBw8eNB6SRnX1dWl8ePHa/369Vd9/emnn9batWu1fv167d+/X5FIRLNnz07cDLe/uN5xkKQ5c+YknR+7dvWvGwHX19erqqpKDQ0Nqq2tVU9PjyoqKtTV1ZXYZiCcD3/KcZCy5HxwWeLb3/62e+SRR5Keu+2229yPf/xjoxXdeCtXrnTjx4+3XoYpSe7ll19OfH3hwgUXiUTcU089lXjus88+c+Fw2D3//PMGK7wxLj8Ozjm3cOFCd88995isx0p7e7uT5Orr651zA/d8uPw4OJc950NWXAmdO3dOBw4cUEVFRdLzFRUV2rdvn9GqbDQ1NamwsFAlJSW6//77dfToUeslmWpublZbW1vSuREMBjVz5swBd25IUl1dnfLz8zV27Fg99NBDam9vt15SRsViMUlSXl6epIF7Plx+HC7JhvMhKyJ06tQp9fb2qqCgIOn5goICtbW1Ga3qxps8ebI2b96s119/XS+88ILa2to0bdo0dXR0WC/NzKV//gP93JCkyspKbdmyRbt379aaNWu0f/9+3Xnnneru7rZeWkY451RdXa3p06ertLRU0sA8H652HKTsOR/63K9yuJbLf7+Qc+6K5/qzysrKxF+XlZVp6tSpuvXWW7Vp0yZVV1cbrszeQD83JGnBggWJvy4tLdXEiRNVXFysnTt3av78+YYry4zFixfrgw8+0N69e694bSCdD190HLLlfMiKK6ERI0Zo0KBBV/yfTHt7+xX/xzOQDB8+XGVlZWpqarJeiplLnw7k3LhSNBpVcXFxvzw/lixZoh07dujNN99M+v1jA+18+KLjcDV99XzIiggNGTJEEyZMUG1tbdLztbW1mjZtmtGq7HV3d+vDDz9UNBq1XoqZkpISRSKRpHPj3Llzqq+vH9DnhiR1dHSopaWlX50fzjktXrxYL730knbv3q2SkpKk1wfK+XC943A1ffZ8MPxQhJdt27a5wYMHu1/+8pfu97//vVu6dKkbPny4O3bsmPXSbphly5a5uro6d/ToUdfQ0ODuvvtuFwqF+v0x6OzsdI2Nja6xsdFJcmvXrnWNjY3uD3/4g3POuaeeesqFw2H30ksvuYMHD7oHHnjARaNRF4/HjVeeXtc6Dp2dnW7ZsmVu3759rrm52b355ptu6tSp7qtf/Wq/Og6PPvqoC4fDrq6uzrW2tiYen376aWKbgXA+XO84ZNP5kDURcs65Z5991hUXF7shQ4a4b33rW0kfRxwIFixY4KLRqBs8eLArLCx08+fPd4cOHbJeVsa9+eabTtIVj4ULFzrnLn4sd+XKlS4SibhgMOhmzJjhDh48aLvoDLjWcfj0009dRUWFGzlypBs8eLAbPXq0W7hwoTt+/Lj1stPqan//ktzGjRsT2wyE8+F6xyGbzgd+nxAAwExWvCcEAOifiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmPm/DjRwNXMkiX4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes()\n",
    "data_ = mnist_train[0]\n",
    "#ax.imshow(data_[0]) \n",
    "ax.imshow(torch.reshape(data_[0],(28,28)))\n",
    "ax.set_title(str(data_[1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = net(data_[0])\n",
    "    sof = nn.Softmax(dim=1)\n",
    "    ans = sof(y)\n",
    "    print(\"原始输入\\n\"+str(data_[0].shape)+\n",
    "        \"\\n原始输出\\n\"+str(y),\n",
    "          \"\\n概率化输出\\n\"+str(ans)+\n",
    "          \"\\n概率求和\\n\"+str(ans.sum())+\n",
    "          \"\\n识别结果\\n\"+str(ans.argmax()))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
