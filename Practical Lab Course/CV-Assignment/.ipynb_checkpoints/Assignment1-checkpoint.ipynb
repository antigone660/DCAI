{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2qDI9QbgO-d"
   },
   "source": [
    "# Topics in Artificial Intelligence\n",
    "# Assignment 1: Intriduction to Deep Learning\n",
    "\n",
    "\n",
    "In Section 1 of the notebook, we have implemented a single-layer supervised neural network for you, so no coding is necessary for this section. Sections 2 and 3 are where your coding expertise is needed, and you'll also be expected to answer specific questions. Section 4 is crafted to familiarize you with PyTorch. Beyond that, its primary purpose is to equip you with essential debugging strategies for training neural networks.\n",
    "\n",
    "# Single-layer neural network\n",
    "This section provides simple implementation of the single-layer supervised neural network that has 4 inputs and 3 outputs. First, let's review the skeleton of a single linear layer neural network. The inputs of the network are the variables $x_1, x_2, x_3, x_4$, or the input vector $\\mathbf{x}=[x_1, x_2, x_3, x_4]$, the outputs of the network are $\\widehat{y}_1,\\widehat{y}_2,\\widehat{y}_3$, or the output vector $\\widehat{\\mathbf{y}}=[$$\\widehat{y}$$_1,\\widehat{y}_2,\\widehat{y}_3]$:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./images/1_layer_net.png\" width=\"450\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "The given $j$-th output $\\widehat{y}_j$ of this single linear layer + activation function is computed as follows:\n",
    "\n",
    "$$\\widehat{y}_j= \\text{sigmoid}(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + w_{4j}x_4 + b_j) = \\text{sigmoid}\\Big(\\sum_{i=1}^{i=4}w_{ij}x_{i} + b_j\\Big)$$\n",
    "\n",
    "In matrix notation, this would be: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  \\widehat{y}_{1} \\\\ \n",
    "  \\widehat{y}_{2} \\\\\n",
    "  \\widehat{y}_{3} \n",
    "\\end{bmatrix}^T=\\mathbf{Sigmoid}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  x_{3} \\\\\n",
    "  x_{4}\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "  w_{1,1} & w_{1,2} & w_{1,3}\\\\\n",
    "  w_{2,1} & w_{3,2} & w_{2,3}\\\\\n",
    "  w_{3,1} & w_{3,2} & w_{3,3}\\\\\n",
    "  w_{4,1} & w_{4,2} & w_{4,3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "  b_{1} \\\\\n",
    "  b_{2} \\\\\n",
    "  b_{3}\n",
    "\\end{bmatrix}^T\\Bigg)\n",
    "\\end{equation}\n",
    "\n",
    "or more compactly:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{\\mathbf{y}}^T = \\mathbf{Sigmoid}(\\mathbf{x}^T \\cdot \\mathbf{W} + \\mathbf{b}^T)\n",
    "\\end{equation}\n",
    "\n",
    "The element-wise sigmoid function is: $\\mathbf{Sigmoid}(\\mathbf{x}) = 1 \\;/\\; (1 + exp(-\\mathbf{x}))$, or alternatively: $\\mathbf{Sigmoid}(\\mathbf{x}) = exp(\\mathbf{x})\\;/\\;(1 + exp(\\mathbf{x}))$. Here the sigmoid is separated logically into an activation layer $\\sigma(x)$ and a linear layer $\\text{linear}(3,4)$ as illustrated in figure. \n",
    "\n",
    "Training these weights $\\mathbf{W}$ and biases $\\mathbf{b}$ requires having many training pairs $(\\widehat{\\mathbf{y}}^{(m)}, \\mathbf{x}^{(m)})$. The inputs $\\mathbf{x}$ can be the pixels of an image, indices of words, the entries in a database, and the outputs $\\widehat{\\mathbf{y}}$ can also be literally anything, including a number indicating a category, a set of numbers indicating the indices of words composing a sentence, an output image itself, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVzTIH-FDFp2"
   },
   "source": [
    "## Forward-propagation\n",
    "\n",
    "Computing the outputs $\\widehat{\\mathbf{y}}$ from the inputs $\\mathbf{x}$ in this network composed of a single linear layer, and a sigmoid layer, is called forward-propagation. Below is the code that implements these two operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q5oXYGAtgO-f",
    "outputId": "fff94757-4e3c-44f7-bd9e-77b36a6cf990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[1] = [[1 2 2 3]]\n",
      "y_hat[1] = [[0.5002601  0.50021727 0.49982597]]\n",
      "\n",
      "x[2] = [[4 5 2 1]]\n",
      "y_hat[2] = [[0.50084826 0.50052724 0.49873652]]\n",
      "\n",
      "x = [[1 2 2 3]\n",
      " [4 5 2 1]]\n",
      "y_hat = [[0.49946884 0.49875937 0.49905569]\n",
      " [0.49938396 0.50126294 0.50084944]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.001\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.001\n",
    "        \n",
    "    # y = Wx + b\n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "\n",
    "# Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y_hat1 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x1))\n",
    "print('x[1] = '+ str(x1))\n",
    "print('y_hat[1] = ' + str(y_hat1) + '\\n')\n",
    "\n",
    "# Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "x2 = np.array([[4, 5, 2, 1]])\n",
    "y_hat2 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x2))\n",
    "print('x[2] = '+ str(x2))\n",
    "print('y_hat[2] = ' + str(y_hat2) + '\\n')\n",
    "\n",
    "# We can also compute both at once, which could be more efficient since it requires a single matrix multiplication.\n",
    "x = np.concatenate((x1, x2), axis = 0)\n",
    "y_hat = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x))\n",
    "print('x = ' + str(x))\n",
    "print('y_hat = ' + str(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYL-G-qkgO-h"
   },
   "source": [
    "## Loss functions.\n",
    "\n",
    "After computing the output predictions $\\widehat{\\mathbf{y}}$ it is necessary to compare these against the true values of $\\mathbf{y}$. Let's call these true, correct, or desired values $\\mathbf{y}$. Typically, a simple loss or cost function is used to measure the degree by which the prediction $\\widehat{\\mathbf{y}}$ is wrong with respect to $\\mathbf{y}$. A common loss function for regression is the sum of squared differences between the prediction and its true value. Assuming a prediction $\\widehat{\\mathbf{y}}^{(d)}$ for our training sample $\\mathbf{x}^{(d)}$ with true value $\\mathbf{y}^{(d)}$, then the loss can be computed as:\n",
    "\n",
    "$$loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)}) = (\\widehat{y}^{(d)}_1 - y^{(d)}_1)^2 + (\\widehat{y}^{(d)}_2 - y^{(d)}_2)^2 + (\\widehat{y}^{(d)}_3 - y^{(d)}_3)^2 = \\sum_{j=1}^{j=3}(\\widehat{y}^{(d)}_j - y^{(d)}_j)^2$$\n",
    "\n",
    "The goal is to modify the parameters [$\\mathbf{W}, \\mathbf{b}$] in the Linear layer so that the value of $loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)})$ becomes as small as possible for all training samples in a set $D=\\{(\\mathbf{x}^{(d)},\\mathbf{y}^{(d)})\\}$. This would in turn ensure that predictions $\\widehat{\\mathbf{y}}$ are as similar as possible to the true values $\\mathbf{y}$. To achieve this we need to minimize the following function:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{W}, \\mathbf{b}) = \\sum_{d=1}^{d=|D|} loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)})$$\n",
    "\n",
    "The only two variables for our model in the function $\\mathcal{L}(\\mathbf{W}, \\mathbf{b})$ are $\\mathbf{W}$ and $\\mathbf{b}$, this is because the training dataset $D$ is fixed. Finding the values of $\\mathbf{W}$ and $\\mathbf{b}$ that minimize the the loss, particularly for complex functions, is typically done using gradient based optimization, like Stochastic Gradient Descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YpscMANsgO-h",
    "outputId": "94879f7e-98ae-48bb-ff02-dd13a261b44d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6271000000000002"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class nn_MSECriterion:  # MSE = mean squared error.\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "\n",
    "# Let's test the loss function.\n",
    "y_hat = np.array([[0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33]])\n",
    "y_true = np.array([[0.25, 0.25, 0.25], [0.33, 0.33, 0.33], [0.77, 0.77, 0.77], [0.80, 0.80, 0.80]])\n",
    "\n",
    "nn_MSECriterion().forward(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAI-ECkbgO-i"
   },
   "source": [
    "\n",
    "## Backward-propagation (Backpropagation)\n",
    "\n",
    "Backpropagation is just applying the chain-rule in calculus to compute the derivative of a function which is the composition of many functions (this is essentially definition of the neural network). \n",
    "\n",
    "Below is the implementation of required derivative computations for our simple network. You are highly advised to derive the derivatives implemented here to make sure you understand how one arrives at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ng5fBmzIgO-j",
    "outputId": "0e8fadd6-ab7f-4fd6-d156-423803c8d056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " num_samples = 1\n",
      "\n",
      "W = [[-0.00929073 -0.01299621  0.02113713]\n",
      " [ 0.00390222  0.00035702 -0.00087313]\n",
      " [-0.01043848 -0.00580328  0.00345056]\n",
      " [-0.00456298  0.00471444 -0.00554558]]\n",
      "B = [[-0.0108167  -0.01342151 -0.00635656]]\n",
      "\n",
      "x1    = [[1 2 2 3]]\n",
      "lin   = [[-0.04686887 -0.02316694  0.00329869]]\n",
      "y_hat = [[0.48828493 0.49420852 0.50082467]]\n",
      "\n",
      "loss = 0.1793305248275684\n",
      "\n",
      "dy_hat = [[0.47656985 0.48841705 0.50164934]]\n",
      "dlin   = [[0.11907706 0.12208788 0.12541199]]\n",
      "dx1    = [[-4.21429106e-05  3.98752188e-04 -1.51875244e-03 -6.63252825e-04]]\n",
      "\n",
      "dW = [[0.11907706 0.12208788 0.12541199]\n",
      " [0.23815411 0.24417576 0.25082399]\n",
      " [0.23815411 0.24417576 0.25082399]\n",
      " [0.35723117 0.36626364 0.37623598]]\n",
      "dB = [[0.11907706 0.12208788 0.12541199]]\n"
     ]
    }
   ],
   "source": [
    "# This is referred above as f(u).\n",
    "class nn_MSECriterion:\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        num_samples = labels.shape[0]\n",
    "        return 2 * (predictions - labels)\n",
    "\n",
    "# This is referred above as g(v).\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # It is usually a good idea to use gv from the forward pass and not recompute it again here.\n",
    "        gv = 1 / (1 + np.exp(-x))  \n",
    "        return np.multiply(np.multiply(gv, (1 - gv)), gradOutput)\n",
    "\n",
    "# This is referred above as h(W, b)\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.01\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        self.gradBias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # dL/dw = dh/dw * dL/dv\n",
    "        self.gradWeight = np.dot(x.T, gradOutput)\n",
    "        # dL/db = dh/db * dL/dv\n",
    "        self.gradBias = np.copy(gradOutput)\n",
    "        # return dL/dx = dh/dx * dL/dv\n",
    "        return np.dot(gradOutput, self.weight.T)\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = [self.weight, self.bias]\n",
    "        gradParams = [self.gradWeight, self.gradBias]\n",
    "        return params, gradParams\n",
    "    \n",
    "\n",
    "# Let's test some dummy inputs for a full pass of forward and backward propagation.\n",
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y1 = np.array([[0.25, 0.25, 0.25]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)  # h(W, b)\n",
    "sigmoid = nn_Sigmoid()  # g(v)\n",
    "loss = nn_MSECriterion()  # f(u)\n",
    "\n",
    "# Forward-propagation.\n",
    "lin = linear.forward(x1)\n",
    "y_hat = sigmoid.forward(lin)\n",
    "loss_val = loss.forward(y_hat, y1) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "dy_hat = loss.backward(y_hat, y1)\n",
    "dlin = sigmoid.backward(lin, dy_hat)\n",
    "dx1 = linear.backward(x1, dlin)\n",
    "\n",
    "print('\\n num_samples = ' + str(y1.shape[0]))\n",
    "\n",
    "# Show parameters of the linear layer.\n",
    "print('\\nW = ' + str(linear.weight))\n",
    "print('B = ' + str(linear.bias))\n",
    "\n",
    "# Show the intermediate outputs in the forward pass.\n",
    "print('\\nx1    = '+ str(x1))\n",
    "print('lin   = ' + str(lin))\n",
    "print('y_hat = ' + str(y_hat))\n",
    "\n",
    "print('\\nloss = ' + str(loss_val))\n",
    "\n",
    "# Show the intermediate gradients with respect to inputs in the backward pass.\n",
    "print('\\ndy_hat = ' + str(dy_hat))\n",
    "print('dlin   = ' + str(dlin))\n",
    "print('dx1    = ' + str(dx1))\n",
    "\n",
    "# Show the gradients with respect to parameters.\n",
    "print('\\ndW = ' + str(linear.gradWeight))\n",
    "print('dB = ' + str(linear.gradBias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "tK_WHjFjgO-k"
   },
   "source": [
    "## Gradient checking \n",
    "\n",
    "The gradients can also be computed with numerical approximation using the definition of derivatives. Let a single input pair $(\\mathbf{x}, \\mathbf{y})$ be the input, for each entry $w_{ij}$ in the weight matrix $\\mathbf{W}$, the partial derivatives can be computed as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial w_{ij}} = \\frac{\\mathcal{L}(\\mathbf{W} + \\mathbf{E}_{ij},b) - \\mathcal{L}(\\mathbf{W} - \\mathbf{E}_{ij}, b)}{2\\epsilon}, $$\n",
    "\n",
    "where $\\mathbf{E}_{ij}$ is a matrix that has $\\epsilon$ in its $(i,j)$ entry and zeros everywhere else. Intuitively this gradient tells how would the value of the loss changes if we change a particular weight $w_{ij}$ by an $\\epsilon$ amount. We can do the same to compute derivatives with respect to the bias parameters $b_i$. Below is the code that checks for a given input $(\\mathbf{x}, \\mathbf{y})$, the gradients for the matrix $\\mathbf{W}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KtpED5azgO-k",
    "outputId": "1bc5f1c3-9672-4632-c117-f46f6a590686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradWeight: [[ -2.93197006  -4.3078065   -5.36027145]\n",
      " [ -4.7613189   -6.99558321  -8.70471432]\n",
      " [-43.15258501 -63.40207518 -78.89220028]\n",
      " [ -6.67837625  -9.81222592 -12.20950719]]\n",
      "\n",
      "approxGradWeight: [[ -2.93206781  -4.3078556   -5.36013332]\n",
      " [ -4.76157665  -6.99571266  -8.70435002]\n",
      " [-43.17372743 -63.41265352 -78.86221655]\n",
      " [ -6.67888331  -9.81248053 -12.2087904 ]]\n"
     ]
    }
   ],
   "source": [
    "# We will compute derivatives with respect to a single data pair (x,y)\n",
    "x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)\n",
    "sigmoid = nn_Sigmoid()\n",
    "criterion = nn_MSECriterion()\n",
    "\n",
    "# Forward-propagation.\n",
    "a0 = linear.forward(x)\n",
    "a1 = sigmoid.forward(a0)\n",
    "loss = criterion.forward(a1, y) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "da1 = criterion.backward(a1, y)\n",
    "da0 = sigmoid.backward(a0, da1)\n",
    "dx = linear.backward(x, da0)\n",
    "\n",
    "gradWeight = linear.gradWeight\n",
    "gradBias = linear.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear.weight)\n",
    "approxGradBias = np.zeros_like(linear.bias)\n",
    "\n",
    "# We will verify here that gradWeights are correct and leave it as an excercise\n",
    "# to verify the gradBias.\n",
    "epsilon = 0.0001\n",
    "for i in range(0, linear.weight.shape[0]):\n",
    "    for j in range(0, linear.weight.shape[1]):\n",
    "        # Compute f(w)\n",
    "        fw = criterion.forward(sigmoid.forward(linear.forward(x)), y) # Loss function.\n",
    "        # Compute f(w + eps)\n",
    "        shifted_weight = np.copy(linear.weight)\n",
    "        shifted_weight[i, j] = shifted_weight[i, j] + epsilon\n",
    "        shifted_linear = nn_Linear(4, 3)\n",
    "        shifted_linear.bias = linear.bias\n",
    "        shifted_linear.weight = shifted_weight\n",
    "        fw_epsilon = criterion.forward(sigmoid.forward(shifted_linear.forward(x)), y) # Loss function\n",
    "        # Compute (f(w + eps) - f(w)) / eps\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / epsilon\n",
    "\n",
    "# These two outputs should be similar up to some precision.\n",
    "print('gradWeight: ' + str(gradWeight))\n",
    "print('\\napproxGradWeight: ' + str(approxGradWeight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dABWMsUgO-l"
   },
   "source": [
    "## Stochastic Gradient Descent.\n",
    "\n",
    "The code below creates a dummy dataset that will be used for training. The inputs are 1000 vectors of size 4, and the outputs are 1000 vectors of size 3. The focus here is on training, however, in a real task one would check accuracy of the model on test (unseen) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mfTGJfLggO-l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 1000\n",
    "\n",
    "# Generate random inputs within some range.\n",
    "x = np.random.uniform(0, 6, (dataset_size, 4))\n",
    "# Generate outputs based on the inputs using some function.\n",
    "y1 = np.sin(x.sum(axis = 1))\n",
    "y2 = np.sin(x[:, 1] * 6)\n",
    "y3 = np.sin(x[:, 1] + x[:, 3])\n",
    "y = np.array([y1, y2, y3]).T\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgSaF_SggO-m"
   },
   "source": [
    "To learn the parameters efficiently we will implement the stochastic gradient descent loop that moves the weights according to the gradients. In each iteration we sample an $(\\mathbf{x}, \\mathbf{y})$ pair and compute the gradients of the parameters, then we update the parameters according to the following gradient descent rules:\n",
    "\n",
    "$$w_{ij} = w_{ij} - \\lambda\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial w_{ij}}$$\n",
    "\n",
    "$$b_i = b_i - \\lambda\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial b_i}$$\n",
    "\n",
    "where $\\lambda$ is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sYqIehlSgO-m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0] = 1.46453202\n",
      "epoch[10] = 1.39850986\n",
      "epoch[20] = 1.38704259\n",
      "epoch[30] = 1.38290154\n",
      "epoch[40] = 1.38062014\n",
      "epoch[50] = 1.37905479\n",
      "epoch[60] = 1.37794413\n",
      "epoch[70] = 1.37711670\n",
      "epoch[80] = 1.37645033\n",
      "epoch[90] = 1.37588711\n",
      "epoch[100] = 1.37540181\n",
      "epoch[110] = 1.37498111\n",
      "epoch[120] = 1.37461528\n",
      "epoch[130] = 1.37429598\n",
      "epoch[140] = 1.37401598\n",
      "epoch[150] = 1.37376914\n",
      "epoch[160] = 1.37355036\n",
      "epoch[170] = 1.37335540\n",
      "epoch[180] = 1.37318077\n",
      "epoch[190] = 1.37302356\n",
      "epoch[200] = 1.37288139\n",
      "epoch[210] = 1.37275224\n",
      "epoch[220] = 1.37263442\n",
      "epoch[230] = 1.37252653\n",
      "epoch[240] = 1.37242736\n",
      "epoch[250] = 1.37233589\n",
      "epoch[260] = 1.37225124\n",
      "epoch[270] = 1.37217267\n",
      "epoch[280] = 1.37209952\n",
      "epoch[290] = 1.37203125\n",
      "epoch[300] = 1.37196735\n",
      "epoch[310] = 1.37190740\n",
      "epoch[320] = 1.37185103\n",
      "epoch[330] = 1.37179792\n",
      "epoch[340] = 1.37174777\n",
      "epoch[350] = 1.37170032\n",
      "epoch[360] = 1.37165534\n",
      "epoch[370] = 1.37161264\n",
      "epoch[380] = 1.37157204\n",
      "epoch[390] = 1.37153336\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.1\n",
    "\n",
    "model = {}\n",
    "model['linear'] = nn_Linear(4, 3)\n",
    "model['sigmoid'] = nn_Sigmoid()\n",
    "model['loss'] = nn_MSECriterion()\n",
    "\n",
    "for epoch in range(0, 400):\n",
    "    loss = 0\n",
    "    for i in range(0, dataset_size):\n",
    "        xi = x[i:i+1, :]\n",
    "        yi = y[i:i+1, :]\n",
    "\n",
    "        # Forward.\n",
    "        a0 = model['linear'].forward(xi)\n",
    "        a1 = model['sigmoid'].forward(a0)\n",
    "        loss += model['loss'].forward(a1, yi)\n",
    "\n",
    "        # Backward.\n",
    "        da1 = model['loss'].backward(a1, yi)\n",
    "        da0 = model['sigmoid'].backward(a0, da1)\n",
    "        model['linear'].backward(xi, da0)\n",
    "\n",
    "        model['linear'].weight = model['linear'].weight - learningRate * model['linear'].gradWeight\n",
    "        model['linear'].bias = model['linear'].bias - learningRate * model['linear'].gradBias\n",
    "    \n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, loss / dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUru8hBmgO-n"
   },
   "source": [
    "# Two-layer neural network with 1-hidden layer.\n",
    "In the previous section, we constructed a single-layer neural network that accepted input vectors of size 4 and produced output vectors of size 3. In this section, your challenge is to adapt the code to train a two-layer network, which includes one hidden layer with a size determined by hidden_state_size. Please note that hidden_state_size is a variable parameter that you should be able to adjust.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"images/2_layer_net.png\" width=\"450\">\n",
    "</center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCtypIohgO-n"
   },
   "source": [
    "## Implementing the network and SGD learning.\n",
    "Please modify the code of Section \"Stochastic Gradient Descent\" to implement a two-layer network and the SGD training procedure for it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Uk5A9pItgO-o"
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 5;\n",
    "\n",
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3JNC9_egO-o"
   },
   "source": [
    "## Gradient checking.\n",
    "Check the gradients of the above network for both linear layer parameters $W_1$ and $W_2$ using some sample input pair ($\\mathbf{x}$, $\\mathbf{y}$). You will likely want to look and model this after Section \"Gradient checking\" above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZQr_l_8igO-o"
   },
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcVHt1uCgO-p"
   },
   "source": [
    "## Activation functions.\n",
    "\n",
    "As was covered in class, there are other activation functions that can be used instead of sigmoid. Implement below the forward and backward operation for two popular activation functions.\n",
    "\n",
    "$$\\text{ReLU}(\\mathbf{x}) = \\text{max}(0, \\mathbf{x})$$\n",
    "\n",
    "$$\\text{Tanh($\\mathbf{x}$)} = \\text{tanh}(\\mathbf{x}) = \\frac{e^{\\mathbf{x}} - e^{-\\mathbf{x}}}{e^{\\mathbf{x}} + e^{-\\mathbf{x}}}$$\n",
    "\n",
    "Note, that in the above activations are applied element-wise on the input vector $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "exffeGbJgO-p"
   },
   "outputs": [],
   "source": [
    "# Rectified linear unit\n",
    "class nn_ReLU:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # Backward pass\n",
    "        \n",
    "# Hyperbolic tangent.\n",
    "class nn_Tanh:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # Backward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPA7lM5NgO-q"
   },
   "source": [
    "## Loss functions.\n",
    "\n",
    "As discussed in class, there are other loss functions that can be used instead of a mean squared error. Implement the forward and backward operations for the following very common loss function where $\\widehat{\\mathbf{y}}$ is a vector of predicted values, and $\\mathbf{y}$ is the vector with ground-truth labels. Assume both vectors are of size $n$. \n",
    "\n",
    "$$\\text{BinaryCrossEntropy}(\\mathbf{y}, \\widehat{\\mathbf{y}}) = - \\frac{1}{n}\\sum_{i=1}^{i=n} [y_i\\text{log}(\\widehat{y}_i) + (1 - y_i)\\text{log}(1 - \\widehat{y}_i)]$$,\n",
    "\n",
    "The binary cross entropy loss assumes that the vector $\\mathbf{y}$ only has values that are either 0 and 1, and the prediction vector $\\widehat{\\mathbf{y}}$ contains values between 0 and 1 (e.g. the output of a $\\text{sigmoid}$ layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dod6reDTgO-q"
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy criterion. Useful for classification as opposed to regression.\n",
    "class nn_BCECriterion:\n",
    "    def forward(self, predictions, labels):\n",
    "        # Forward pass.\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        # Backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iS6liR3gO-r"
   },
   "source": [
    "<b>Optional</b>: Most deep learning libraries support batches, meaning you can forward, and backward a set of inputs. So far the code supports batches in the forward pass. However, the backward pass does not support batches. Modify the code in backward function of the nn_Linear class to support batches. Then test the implementation by training the network in Section 2.1 using a batch size of 32. <span style=\"color:#666\">(Keep in mind that the gradWeight and gradBias vectors should accumulate the gradients of each sample in the batch. This is because the gradient of the loss with respect to the batch is the sum of the gradients with respect to each sample in the batch. This means that for a batch of size 32, the sum will be over the 32 samples).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QohUVpGgO-r"
   },
   "source": [
    "# Behaviour of neural networks.\n",
    "\n",
    "Prior to this section, all experiments were done in a dummy dataset where it is difficult to see how neural networks behave on more realistic data. In this section the goal is to get a feel for how newural networks behave and what effect hidden statest may play. Below is the code that generates and visualizes a classification dataset of 400 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JJ2BDhMLgO-s"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.datasets\n",
    "\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400                # number of examples\n",
    "    N = int(m/2)           # number of points per class\n",
    "    D = 2                  # dimensionality\n",
    "    X = np.zeros((m,D))    # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4                  # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2                    # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Load the data\n",
    "X, Y = load_planar_dataset();\n",
    "\n",
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y[0, :], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3ElKAWtgO-s"
   },
   "source": [
    "## Two-layer neural network with 1-hidden layer of size = 1.\n",
    "\n",
    "Reimplement the network from Section \"Implementing the network and SGD learning\" here, train it and then display the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LmYvxjj7gO-s"
   },
   "outputs": [],
   "source": [
    "# Build a two-layer neural network (so one hidden layer) with sigmoid activations \n",
    "# and MSE loss. The hidden_state_dimensionality should be set to 1 using the variable\n",
    "# below.\n",
    "hidden_state_size = 1; \n",
    "\n",
    "# Define the 2-layer network here (fill in yout code)\n",
    "\n",
    "\n",
    "# Optimize the parameters of the neural network using stochastic gradient descent\n",
    "# using the following parameters\n",
    "\n",
    "learningRate = 0.01\n",
    "numberEpochs = 300\n",
    "\n",
    "for epoch in range(0, numberEpochs):\n",
    "    loss = 0\n",
    "    for i in range(0, Y.size):        \n",
    "        # Forward pass (fill in your code)\n",
    "       \n",
    "        # Backward pass (fill in your code)\n",
    "        \n",
    "        # Update gradients (fill in your code)\n",
    "\n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, loss / dataset_size))\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "classEstimate = np.zeros((400,1), dtype='uint8')\n",
    "\n",
    "for i in range(0, 400):        \n",
    "    # Forward pass (fill in your code)\n",
    "       \n",
    "        \n",
    "    classEstimate[i,0] = (y_hat > 0.5)\n",
    "\n",
    "plt.scatter(X[0, :], X[1, :], c=classEstimate[:,0], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwbxbjZLgO-t"
   },
   "source": [
    "## Two-layer neural network with 1-hidden layer of size = 5.\n",
    "\n",
    "Redo the experiment with the hidden layer of size 5 and visualize the result. <b>Describe in a separate cell of the notebook what is different between the two runs </b>. What behaviout did network with largerr hidden state exhibit that the one with smaller one did not? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PpHIRfG9gO-t"
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 5; \n",
    "\n",
    "# Rest should be taken from above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ap_IG-ztSn7"
   },
   "source": [
    "# PyTorch and Debugging.\n",
    "\n",
    "In this section the goal is to experiment with PyTorch and also with the typical debugging techniques used in deep learning and simple debugging tools available to you. In this part, you are given a re-implementation of the dataset and neural network you worked with in the previous section of the assignment. However, this re-implementation is based on the data structures and functions available in PyTorch. Note that both the data loader and the model code contain a few strategically placed **bugs**. Your goal in this part of the assignment is to use the provided strategies to discover those bugs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-QHKwuANBcO"
   },
   "source": [
    "## Dataloader \n",
    "\n",
    "First thing that typically happens in PyTorch code is definition of a dataloader. A dataloader is used to load and organize the data for training, validation and testing. We are providing a slightly buggy implementation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2k2I2QvowZ9E"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class PlanarDataset(data.Dataset):\n",
    "    def __init__(self,num_examples=400):\n",
    "        ## Function to initialize the dataset class\n",
    "        np.random.seed(1)\n",
    "        m = 400                  # number of examples\n",
    "        N = int(m/2)             # number of points per class\n",
    "        D = 2                    # dimensionality\n",
    "        a = 4                    # maximum ray of the flower\n",
    "\n",
    "        self.X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "        self.Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "\n",
    "        for j in range(2):\n",
    "            ix = range(N*j,N*(j+1))\n",
    "            t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "            r = a*np.sin(4*t) + np.random.randn(N)*0.2                    # radius\n",
    "            self.X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "            self.Y[ix] = random.randint(0, 1) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## Return a data point given an index. Convert numpy array to Pytorch Tensor.\n",
    "        return torch.from_numpy(self.X[index,:],).type(torch.FloatTensor), torch.from_numpy(self.Y[index,:]).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        ## Return the length of the dataset\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def get_all_data(self):\n",
    "      ## Return all the data. Not something you would do for large datasets. Just used here for convenience of visualization\n",
    "      return self.X.T, self.Y.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLPWDvCqJH0q"
   },
   "source": [
    "## **Debugging Strategy 1**: Always visualize the data.\n",
    "\n",
    "Please use the code below to visualize the data from the dataloader above. Please run this cell multiple times. You should see that the visualization changes. This means that something non-determenistic is happening in the datalader. You should be able spot what is happening from visualization. Once you do, go back and fix the data loader above. \n",
    "\n",
    "**Hint:** only one line needs changing in the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVDtrhWYL-0g"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "planarDataObj = PlanarDataset()\n",
    "X, Y = planarDataObj.get_all_data()\n",
    "\n",
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y[0, :], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frNnA41BOEFD"
   },
   "source": [
    "## Model definition and optimization\n",
    "\n",
    "Functions bellow define a version of neural network with a single hidden layer, from Section \"Behaviour of neural networks\", using PyTorch functions. Note that similarly to above this code is not bug-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alCDsgiyyGiB"
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 5; \n",
    "\n",
    "# Define a model and loss \n",
    "linear1 = nn.Linear(in_features=2, out_features=hidden_state_size)\n",
    "for param in linear1.parameters():  \n",
    "    param.requires_grad = False\n",
    "sigmoid = nn.Sigmoid()\n",
    "linear2 = nn.Linear(in_features=hidden_state_size, out_features=1) \n",
    "model  = nn.Sequential(linear1,sigmoid,linear2,sigmoid)\n",
    "\n",
    "# Define a loss \n",
    "MSELoss = nn.MSELoss()\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 300\n",
    "batchsize = 64\n",
    "\n",
    "# Setup the optimizer \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Seup the dataset and dataloaders (one for training and one for testing)\n",
    "planarDataObj = PlanarDataset()\n",
    "dataset_size = planarDataObj.__len__()\n",
    "train_loader = torch.utils.data.DataLoader(planarDataObj, batch_size=batchsize, shuffle=True) ## Shuffle will randomly shuffle the data\n",
    "test_loader = torch.utils.data.DataLoader(planarDataObj, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        # Get data for the minibatch\n",
    "        input,target = data\n",
    "        \n",
    "        # We have to set gradients to zero at the start of every iteration\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Forward pass through the model\n",
    "        output = model(input)\n",
    "        loss = MSELoss(output,target)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss per epoch    \n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, running_loss / dataset_size))\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "classEstimate = np.zeros((400,1), dtype='uint8')\n",
    "all_data = np.zeros((400,2))\n",
    "\n",
    "start_ind, end_ind = 0,0\n",
    "\n",
    "# Testing loop\n",
    "for i, data in  enumerate(test_loader):\n",
    "    # Get data for the minibatch\n",
    "    input, target = data\n",
    "    num_samples = input.shape[0]\n",
    "    \n",
    "    # Forward pass through the trained model\n",
    "    output = model(input)\n",
    "\n",
    "    # Convert torch tensor output to numpy tensor\n",
    "    output = output.detach().numpy()\n",
    "\n",
    "    # Bookeeping on predictions\n",
    "    end_ind = start_ind + num_samples\n",
    "    all_data[start_ind:end_ind,:] = input.detach().numpy()\n",
    "    classEstimate[start_ind:end_ind,0] = (output.squeeze() > 0.5)\n",
    "    start_ind = end_ind\n",
    "\n",
    "# Plot the result\n",
    "all_data = all_data.T\n",
    "plt.scatter(all_data[0, :],all_data[1, :], c=classEstimate[:,0], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzF-GfZbS5FH"
   },
   "source": [
    "## **Debugging Strategy 2:** Remove all sources of randomness from the code\n",
    "\n",
    "Note that while it is clear that the code above isn't performing as expected (loss does not go down) with every run of the cell you should see a slightly different result and order of loss values. This is because of inherent randomness in the code. While the randomness is good when learning (avoids biases) it is bad when debugging for obvious reasons. Modify the code above to make sure it executes the same way every time you run it. For this you will want to make use of the function below and also fix the randomness in dataloader. \n",
    "\n",
    "**Note:** Use the function below as well as look at ensuring that the order of data in the dataloader remains fixed. You may want to consult PyTorch guidelines on reproducibility https://pytorch.org/docs/stable/notes/randomness.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EEIGzLht1-0c"
   },
   "outputs": [],
   "source": [
    "def fix_seeds():\n",
    "    random_seed = 1      # or coluld be any of your favorite number \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "\n",
    "# Copy the code from Section Model definition and optimization and modify accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AU79m3mA3Mmm"
   },
   "source": [
    "## **Debugging Strategy 3:** Train with minimal amount of data \n",
    "\n",
    "It may still be difficult to identify what is wrong based on results in Section \"Remove all sources of randomness from the code\". A really good strategy, especially for model debugging, is to try optimizing with a single example/sample. Modify the code from Section \"Remove all sources of randomness from the code\" in order to do this. Once you do and run it, it should be obvious that there is a bug in your code and what that bug might be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jKNy5BWA4EzF"
   },
   "outputs": [],
   "source": [
    "# Copy the code from Section \"Remove all sources of randomness from the code\" and modify to run with batch size of 1 and only one / same batch used in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp-4-gESCP9k"
   },
   "outputs": [],
   "source": [
    "# Once you find the bug, fix it here and make sure you can effectively reproduce the result in Section \"Behaviour of neural networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5ggif0e4TSb"
   },
   "source": [
    "## **Debugging Strategy 4:** Use Tensorboard or similar.\n",
    "\n",
    "At this point the model and training code should be correct and you should be able to train the model. However, the hyperparameters may still not be optimal. For hyperparameter tuning and to log many experiments it is advisable to use some way to visualise the training process. Tensorboard provides such ability. Another alternative is Weights & Biases.  \n",
    "\n",
    "Let’s now try using TensorBoard with PyTorch! Before logging anything, we need to create a SummaryWriter instance. Writer will output to ./runs/ directory by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Icqw80OZ-Bdj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu0LTjEi8EzP"
   },
   "source": [
    "We can use `add_scalar(tag, scalar_value, global_step=None, walltime=None)` to log scalar values (e.g. loss). We have to call `flush()` method to make sure that all pending events have been written to disk. Finally if we do not need the summary writer anymore, then call `close()` method.\n",
    "\n",
    "Your next task is to slightly modify the code to log the Loss and Accuracy into the Tesnorboard log file. This will require modifying the training loop as follows:\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        # Get data for the minibatch\n",
    "        input,target = data\n",
    "\n",
    "        # We have to set gradients to zero at the start of every iteration\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(input)\n",
    "        loss = MSELoss(output,target)\n",
    "        running_loss += loss.item()\n",
    "        total_correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Log accuracy and loss\n",
    "    writer.add_scalar(\"Loss/train\", running_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", total_correct/ dataset_size, epoch)\n",
    "        \n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, running_loss / dataset_size))\n",
    "\n",
    "# Flush and close the log file\n",
    "writer.flush()\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1Qy0VVN-HuY"
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGPK-4_I7t8s"
   },
   "source": [
    "Now install the tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOAJcbKT7zIL"
   },
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqZkEr5a71Mv"
   },
   "source": [
    "Now to start tensorboard in google colab we can run following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV4z75Ak-fVO"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
