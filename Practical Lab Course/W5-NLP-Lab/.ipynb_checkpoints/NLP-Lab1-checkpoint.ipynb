{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fecaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746bef1",
   "metadata": {},
   "source": [
    "## Movie Review Classifier üçøüìΩÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58cd71",
   "metadata": {},
   "source": [
    "In this lab session, we'll be training a model to classify movie reviews as 'good' or 'bad.'\\\n",
    "The data consists of 50,000 real move reviews from IMBD.\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6bc9b",
   "metadata": {},
   "source": [
    "We'll load the data as a zipped csv.¬†\\\n",
    "Notice that `pd.read_csv()` can take a URL as the path argument and that we can read in a compressed file without first expanding it if we specify the `compression` format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d14744",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = './data/movie_reviews.zip'\n",
    "df = pd.read_csv(data_url, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fd2bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb303695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fae19e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de24367",
   "metadata": {},
   "source": [
    "We see that the dataset consists of text reviews and binary labels. Intuitively, the positive class is \"good\" while the negative is \"bad.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51a555",
   "metadata": {},
   "source": [
    "Here are two examples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14999480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'label: bad'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Im a die hard Dads Army fan and nothing will ever change that. I got all the tapes, DVD\\'s and audiobooks and every time i watch/listen to them its brand new. <br /><br />The film. The film is a re run of certain episodes, Man and the hour, Enemy within the gates, Battle School and numerous others with a different edge. Introduction of a new General instead of Captain Square was a brilliant move - especially when he wouldn\\'t cash the cheque (something that is rarely done now).<br /><br />It follows through the early years of getting equipment and uniforms, starting up and training. All in all, its a great film for a boring Sunday afternoon. <br /><br />Two draw backs. One is the Germans bogus dodgy accents (come one, Germans cant pronounced the letter \"W\" like us) and Two The casting of Liz Frazer instead of the familiar Janet Davis. I like Liz in other films like the carry ons but she doesn\\'t carry it correctly in this and Janet Davis would have been the better choice.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'label: good'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = {0: 'bad', 1: 'good'}\n",
    "seen = {'bad': False, 'good': False}\n",
    "for i in range(df.shape[0]):\n",
    "    label = df.loc[i,'label']\n",
    "    if not seen[labels[label]]:\n",
    "        # display/print combination used to appease Ed's strange output behavior\n",
    "        display(df.loc[i, 'text'])\n",
    "        print()\n",
    "        display(f\"label: {labels[label]}\")\n",
    "        print()\n",
    "        seen[labels[label]] = True\n",
    "    if all(val == True for val in seen.values()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee4ceb",
   "metadata": {},
   "source": [
    "**Some Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9326a9b",
   "metadata": {},
   "source": [
    "In the 2nd example, we can see some html tags inside the review text.\n",
    "\n",
    "Complete the `remove_br()` function by providing its call to `re.sub()` with a regex that removes those pesky \"\\<br />\" tags from an input string, `x`.\\\n",
    "Speciffically, we should replace 2 consecutive occurances of \"\\<br />\" with a single space (can you see why?).\n",
    "\n",
    "**Hint:** It is good practice to use 'raw' string when writing regular expressions to ensure that special characters are treated correctly. Raw strings are appended with an 'r' like this: `r'this is a raw string'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b94fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# fill in the regular expression\n",
    "remove_br = lambda x: re.sub(___, ' ', x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4edfe",
   "metadata": {},
   "source": [
    "Use the dataframe's `apply()` method to apply `remove_br` to each review in both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9893c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "df['text'] = _______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05013853",
   "metadata": {},
   "source": [
    "And we can see that the tags have been removed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30134871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[4,'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4770d",
   "metadata": {},
   "source": [
    "Don't worry about any newline characters or backslashes you may see before apostrophes in the examples above. This is just a quirk of how Jupyter displays strings by default.\\\n",
    "We don't see that these characters if we explicitly `print` the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fe245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_str = df.loc[4,'text']\n",
    "print(example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e105db9",
   "metadata": {},
   "source": [
    "We'll continue our preprocessing by next **removing punctuation**.\\\n",
    "But first, let's keep a copy of the data *with* punctuation. This will be useful at the end of the notebook when we want to display the original text of specific observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e3b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store copy of data with punctuation\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa28d4",
   "metadata": {},
   "source": [
    "The next regex we need is a bit more involved.\\\n",
    "**This should match any non-whitespace, any non-alphanumeric characters, and underscores** (strangly, underscores are not covered by the first 2 conditions).\n",
    "\n",
    "**Hints:**\n",
    "- `\\w` matches alphanumeric characters\n",
    "- `\\s` matches whitespace\n",
    "- `[]` can be used to denote a set of characters. ex: `r'[ab]'` will match on 'a' *or* 'b'\n",
    "- `^` at the beginning of a character set denotes *negation*. ex: `r'[^0-9]'` will matching any non-integer\n",
    "- `|` is the *logical or* operator. ex: `r'cat|dog'` will match the strings 'cat' *or* 'dog' \n",
    "- There are many helpful sites for testing regexes. [Here's a nice one](https://www.regextester.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c714b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# create a regex that will match the characters described above \n",
    "punc_regex = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b57e39",
   "metadata": {},
   "source": [
    "Here we'll use an alternative to the `apply` approach we saw above.\\\n",
    "Pandas has its own set of built-in string methods which includes a version of `replace`. But unlike Python's `str.replace()` this can actually use regexes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a874d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.text.str.replace(punc_regex, '', regex=True) # remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cbd2e",
   "metadata": {},
   "source": [
    "If all went well we can see that punctuation has been removed from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a82b9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_str = df.loc[4,'text']\n",
    "print(example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d4105",
   "metadata": {},
   "source": [
    "**Train/Test Split**\n",
    "\n",
    "Rather than splitting the data directly with `train_test_split` we'll instead use it to generate indices for the train and test data.\\\n",
    "This may seem strange, but there is a good reason for it. These indices will later allow us to recover the original, unprocessed text from `df_raw` for any given training and test observations. \n",
    "\n",
    "Notice too that we are stratifying on the label. This will help ensure that good and bad reviews appear in the same proportions in both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41b691ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate indices to designate train and test observations\n",
    "train_idx, test_idx = train_test_split(range(df.shape[0]), test_size=0.2, random_state=0, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a22a946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the predictor from the response\n",
    "x = df.text.values\n",
    "y = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e3d9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets using the generated indices\n",
    "x_train = x[train_idx]\n",
    "y_train = y[train_idx]\n",
    "x_test = x[test_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886b9b5",
   "metadata": {},
   "source": [
    "**Building the Classifier Pipeline**\\\n",
    "**Step 1: Vectorizor**\n",
    "\n",
    "It's true that there are still several preprocessing steps to be done such as converting to lowercase and tokenizing the reviews, but these can be done for using sklearn's [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d88b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df712db2",
   "metadata": {},
   "source": [
    "Instantiate a `TfidfVectorizer` with parameters such that it will:\n",
    "- set all reviews to lowercase\n",
    "- remove english stopwords\n",
    "- exclude words that occur in less than 1 review in 10,000\n",
    "- exclude words that occur in more than 90% of reviews\n",
    "\n",
    "**Hint:** Reading the documentation, you'll see the arguments you need are `lowercase`, `stop_words`, `min_df`, and `max_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e236cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "vec = TfidfVectorizer(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24c836",
   "metadata": {},
   "source": [
    "**Step 2: Classifier**\n",
    "\n",
    "We'll use logistic regression with l2 regularization as our classifier model. The [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html?highlight=logisticregressioncv#sklearn.linear_model.LogisticRegressionCV) object allows us to easily tune for the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078f881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f66e5e",
   "metadata": {},
   "source": [
    "With 40,000 training observations and each word in the vectorizer's vocabulary counting acting as a predictor training could be slow.\\\n",
    "This issue is exacerbated when using cross validation as we need fit the model multiple times!\\\n",
    "We'll set our classifier CV parameters so as to help keep the training time down to around 30 seconds or so.\\\n",
    "- l2 penalty (e.g., Ridge)\n",
    "- 10 iterations per fit (remember, logistic regression has no closed form solution for the betas!)\n",
    "- 5-fold CV\n",
    "- random state of 0 (the fitting can be stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "317752aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Instantiate our Classifier\n",
    "clf = LogisticRegressionCV(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef7e1f",
   "metadata": {},
   "source": [
    "**Step 3: Pipeline**\n",
    "\n",
    "Any text data going into our classifier will have to first be converted to numerical data by our vectorizer.\\\n",
    "One way to do this would be to:\n",
    "1. fit the vectorizor on the training data\n",
    "2. transform a dataset with the fitted vectorizer\n",
    "3. pass the transformed data to the classifier\n",
    "\n",
    "(1) only needs to be done once, but (2) & (3) would need to be done manually for train, test, and any other data we want to give them model.\\\n",
    "This would be tedious! Luckily, sklearn's [Pipline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline) object allow use to connect one more 'transformers' (such as a scaler or vectorizer) with a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7897ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be90c9",
   "metadata": {},
   "source": [
    "Use [make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=make_pipeline#sklearn.pipeline.make_pipeline) to connect the vectorizor, `vec`, and our classifier, `clf`, into a single pipeline.\n",
    "\n",
    "**Hint:** You can set `verbose=True` to see the individual steps during the fit process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f7a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Construct the pipeline\n",
    "pipe = make_pipeline(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe0b5d",
   "metadata": {},
   "source": [
    "**Step 4: Fitting**\n",
    "\n",
    "When it comes to fitting, we can treat the pipeline object as if it were the classifier object itself, and simply call `fit` on the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a3d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of time, we are fitting quickly and we may not converge\n",
    "# We'll supress those pesky warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# We also ignore FutureWarnings due to version issues on Ed\n",
    "simplefilter(\"ignore\", category=(ConvergenceWarning, FutureWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95a8355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_fit) ###\n",
    "# Fit the model via the pipeline\n",
    "pipe.___(___,___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e449d2",
   "metadata": {},
   "source": [
    "We can inspect the steps of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feb94be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()['steps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41580761",
   "metadata": {},
   "source": [
    "By default they are named using the all lowercase class name of each object.\\\n",
    "We can use these names to access the fitted objects inside. Here we see the size of our vectorizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccca2cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = pipe.get_params()['tfidfvectorizer'].get_feature_names()\n",
    "print('# of features:', len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706fcf1",
   "metadata": {},
   "source": [
    "There are too many to print, but we can peek at a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "929a8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 40\n",
    "feature_sample_idx = np.random.choice(len(features), size=sample_size, replace=False)\n",
    "print(np.array(features)[feature_sample_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3f2e8",
   "metadata": {},
   "source": [
    "Similarly, we can access the fitted logistic model and see what regularization parameter was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86399e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_C = pipe.get_params()['logisticregressioncv'].C_[0]\n",
    "print(f'Best C from cross-validation: {best_C:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab190d",
   "metadata": {},
   "source": [
    "**Step 5: Prediction**\n",
    "\n",
    "Just like we did when fitting, we can treat the pipeline object as the classifier when making predictions.\\\n",
    "Predict on the test data to get:\n",
    "1. class labels\n",
    "2. probabilities of being the positive class (i.e., 'good' reviews)\n",
    "3. test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b0d084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Predict class labels on test data\n",
    "y_pred = pipe.___(___)\n",
    "\n",
    "# Predict probabilities of the positive on the test data\n",
    "y_pred_proba = pipe.___(___)[___,___]\n",
    "\n",
    "# Calculate test accuracy (there are several ways to do this)\n",
    "test_acc = ___\n",
    "print(f\"test accuracy: {test_acc:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3f289",
   "metadata": {},
   "source": [
    "Can you get better than 0.896 by tweaking the preprocessing, or vetorizer and classifier parameters? Perhaps inspecting how our model makes its predictions may help us decide how we might improve the model in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0203db",
   "metadata": {},
   "source": [
    "**Step 6: Interpretation**\n",
    "\n",
    "Below we'll use the `eli5` library to have some fun interpreting what is driving our model's predictions on specific test observations.\n",
    "\n",
    "- [ELI5](https://eli5.readthedocs.io/en/latest/) is a Python library which allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65b9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Install ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb097024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interpretation\n",
    "import eli5\n",
    "# for parsing/formating eli5's HTML output\n",
    "from bs4 import BeautifulSoup\n",
    "# for displaying formatted HTML output\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8566b4",
   "metadata": {},
   "source": [
    "Here are the words driving positive class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "303414ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(clf, vec=vec, top=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec2b7a",
   "metadata": {},
   "source": [
    "Hmm, those digits like 710, 810, and 410 driving predictions seems strange. What might they represent?\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0d164",
   "metadata": {},
   "source": [
    "We'll use the 'raw' data with punctuation when inspecting the data (See! It is coming in handy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3289273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw = df_raw.text[train_idx].values\n",
    "x_test_raw = df_raw.text[test_idx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08afdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df.text.str.contains(' 710 ')].iloc[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfea85",
   "metadata": {},
   "source": [
    "These are actually numerical ratings embedded in the reviews! Looking at the text without the punctuation made it hard for us to see this at first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2fcd6",
   "metadata": {},
   "source": [
    "Here's a helper function used to remove some extraneous things from `eli5`'s output. We just want to see the highlighted text.\\\n",
    "You don't need to read through the function but it is here as a nice resource/example. ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eafdf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eli5_html(clf, vec, observation):\n",
    "    \"\"\"\n",
    "    helper function for nicely formatting and displaying eli5 output\n",
    "    \"\"\"\n",
    "    # Get info on is driving a given observation's predictions\n",
    "    eli5_results = eli5.show_prediction(estimator=clf, doc=observation, vec=vec, targets=[True], target_names=['bad', 'good'])\n",
    "    # Convert eli5's HTML data to BS object for parsing/formatting\n",
    "    soup = BeautifulSoup(eli5_results.data, 'html.parser')\n",
    "    # Remove a table we don't want\n",
    "    soup.table.decompose()\n",
    "    # Remove the first <p> tag with unwanted text\n",
    "    soup.p.decompose()\n",
    "    # Display the newly formatted HTML!\n",
    "    display(HTML(str(soup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026c04a",
   "metadata": {},
   "source": [
    "Now all you need to do is find the specific observations requested.\\\n",
    "You'll need your `y_pred_proba` values for this section to find which elements from `x_test_raw` to select.\n",
    "\n",
    "**Hint:** [np.argsort()](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html), [np.flip()](https://numpy.org/doc/stable/reference/generated/numpy.flip.html?highlight=flip#numpy.flip), and [np.abs()](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html) may be useful here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffa9cf",
   "metadata": {},
   "source": [
    "### What are the **5 worst** movie reviews in the test set according to your model? üçÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d18f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Find indices of 5 worst reviews\n",
    "worst5 = x_test_raw[___]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1130af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, review in enumerate(worst5):\n",
    "    style = 'background-color:black;color:white;font-weight:bold;padding:4px'\n",
    "    display(HTML(f\"<p style={style}>Bad Movie #{i+1} üçÖ</p>\"))\n",
    "    eli5_html(clf, vec, review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b746bf",
   "metadata": {},
   "source": [
    "### What are the **5 best** movie review in the test set according to your model? üèÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5931802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Find indices of 5 best reviews\n",
    "best5 = x_test_raw[___]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1250cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, review in enumerate(best5):\n",
    "    display(HTML(f\"<p style={style}>Good Movie #{i+1} üèÜ</p>\"))\n",
    "    eli5_html(clf, vec, review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f479a",
   "metadata": {},
   "source": [
    "What are the **5 most 'meh'** movie review in the test set according to your model? üòê\\\n",
    "That is, which reviews are the most neutral according to your model?\\\n",
    "Upon reading some of these reviews you may find their sentiment to actually *not* be very ambiguous. What might be confusing our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae42fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please fill this code block!\n",
    "# Find indices of the 5 most neutral reviews\n",
    "meh5 = x_test_raw[___]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cccc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, review in enumerate(meh5):\n",
    "    display(HTML(f\"<p style={style}>'Meh' Movie #{i+1} üòê</p>\"))\n",
    "    eli5_html(clf, vec, review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c58acd",
   "metadata": {},
   "source": [
    "Despite some difficulties with a few of the 'meh' movies, our model is actually pretty good! In fact, it works so well you can actually use it to find _mistakes_ in the manually labeled data!\\\n",
    "This can be done by inspecting which training observation predictions differ the most from the provided labels.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d659c3",
   "metadata": {},
   "source": [
    "**Write your own review**\n",
    "\n",
    "Finally, you can try writing a review of your own and see what your model does with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf078e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_review = \"\"\"\n",
    "            your review here\n",
    "            \"\"\"\n",
    "\n",
    "# Remove punctuation using your regex from earlier\n",
    "my_review = re.sub(punc_regex, '', my_review)\n",
    "# Remove leading & trailing whitespace\n",
    "# and put into a numpy array (which the model expects)\n",
    "my_review = np.array([my_review.strip()])\n",
    "my_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abdc48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_review_proba = pipe.predict_proba(my_review)[:,1][0]\n",
    "my_review_label = pipe.predict(my_review)[0]\n",
    "print('predicted class:', my_review_label)\n",
    "print('predicted probability:', my_review_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8de285ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"<p style={style}>My Review üçø</p>\"))\n",
    "eli5_html(clf, vec, my_review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57094c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4db4803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
